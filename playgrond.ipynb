{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(\n",
      "    logits: tensor([[0.2432, 0.2141, 0.2676, 0.2751]], device='cuda:1',\n",
      "                   grad_fn=<SoftmaxBackward0>),\n",
      "    act: tensor([3], device='cuda:1'),\n",
      "    state: None,\n",
      "    dist: Categorical(probs: torch.Size([1, 4])),\n",
      ")\n",
      "0.060961783\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from tianshou.data import Collector\n",
    "from tianshou.env import SubprocVectorEnv\n",
    "from tianshou.policy import PPOPolicy\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.discrete import Actor, Critic\n",
    "from tqdm import tqdm\n",
    "import tianshou\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "DEVICE = \"cuda:1\"\n",
    "# 定义环境\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "state_shape = env.observation_space.shape\n",
    "action_shape = env.action_space.n\n",
    "net = Net(state_shape, hidden_sizes=[1024] * 2, device=DEVICE)\n",
    "actor = Actor(net, action_shape, device=DEVICE)\n",
    "critic = Critic(net, device=DEVICE)\n",
    "\n",
    "dist_fn = torch.distributions.Categorical\n",
    "\n",
    "\n",
    "policy = PPOPolicy(\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    dist_fn=dist_fn,\n",
    "    optim=None,\n",
    "    discount_factor=0.99,\n",
    "    max_grad_norm=0.5,\n",
    "    eps_clip=0.2,\n",
    "    vf_coef=0.5,\n",
    "    ent_coef=0.01,\n",
    "    reward_normalization=True,\n",
    "    action_space=env.action_space,\n",
    "    action_scaling=False,\n",
    "    deterministic_eval=True,\n",
    ").to(DEVICE)\n",
    "policy.eval()\n",
    "possible_actions = torch.arange(env.action_space.n)\n",
    "def get_loss(env, policy, obs):\n",
    "    if isinstance(policy, PPOPolicy):\n",
    "        assert isinstance(\n",
    "            env.action_space, gym.spaces.Discrete\n",
    "        ), \"Only discrete action spaces supported for loss function\"\n",
    "\n",
    "        obs = torch.tensor(obs, dtype=torch.float32).to(DEVICE)\n",
    "        batch = tianshou.data.Batch(obs=obs.unsqueeze(0),info=\"\")\n",
    "        with torch.no_grad():\n",
    "            out = policy(batch)\n",
    "        probs = out.logits.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "        return probs.max() - probs.min()\n",
    "\n",
    "    raise NotImplementedError(f\"Model type {type(policy)} not supported\")\n",
    "\n",
    "\n",
    "obs, _ = env.reset()\n",
    "batch = tianshou.data.Batch(obs=obs.reshape(1,-1), info=\"\")\n",
    "print(policy.forward(batch))\n",
    "print(get_loss(env,policy,obs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

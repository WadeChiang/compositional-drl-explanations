{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove useless neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 2)\n",
      "[21  9  1]\n",
      "[ -0.3050744   -3.3005354   -0.19211519  -0.55337304  -0.6725486\n",
      "  -0.48471168  -0.81506175  -9.164504    -0.97856236   0.6999247\n",
      "  -1.8707147   -4.10077     -0.99064034  -0.7340159   -1.4050695\n",
      "  -0.24971282  -1.3867046   -0.61150247  -2.8810499   -7.1059012\n",
      "   2.008685    -5.9896917   -0.5528022    0.21797323  -5.7041044\n",
      "  -0.3118731   -0.20818149  -3.7404547   -9.375502    -8.10238\n",
      "  -1.0194423   -0.5091292   -6.633438    -9.479743    -7.7186375\n",
      "   1.7107706   -0.52075315  -5.111542    -0.50309795  -0.75730795\n",
      "  -4.242246    -0.44974715  -0.84749633  -0.94891894  -0.26672843\n",
      "  -0.5260327   -1.7301596   -1.975936     2.5473194  -10.748297\n",
      "  -1.6153109   -3.9273074   -1.0840628   -4.644953    -4.5544505\n",
      "  -3.9086409   -1.4812176   -8.031258    -0.54643714  -0.66253084\n",
      "  -0.62048554  -0.31940624  -1.1657099  -11.956685  ]\n",
      "[[26.84132  13.382536]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "weights = np.load('weights.npy')\n",
    "hidden_outputs = np.load('hidden_outputs.npy')\n",
    "states = np.load('observations.npy')\n",
    "actions = np.load('actions.npy')\n",
    "print(weights.shape)\n",
    "index = states[:,0]==21\n",
    "states = states[index]\n",
    "hidden_outputs = hidden_outputs[index]\n",
    "actions = actions[index]\n",
    "print(states[0])\n",
    "print(hidden_outputs[0])\n",
    "print(hidden_outputs[0].reshape(1,64)@weights)\n",
    "print(actions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tianshou.data import Collector, VectorReplayBuffer, Batch\n",
    "from tianshou.utils.net.common import Net\n",
    "from gymnasium.wrappers import TransformObservation\n",
    "from tianshou.env import SubprocVectorEnv\n",
    "from tianshou.policy import DQNPolicy\n",
    "import pprint\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"Blackjack-v1\", natural=False, sab=False)\n",
    "    env = TransformObservation(env, lambda obs: np.array(obs), observation_space=None)\n",
    "    return env\n",
    "\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "env = make_env()\n",
    "state_shape = 3\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "\n",
    "q_net = Net(state_shape, action_shape, hidden_sizes=[64] * 2, device=DEVICE).to(DEVICE)\n",
    "optim = torch.optim.Adam(q_net.parameters(), lr=2.5e-4)\n",
    "policy = DQNPolicy(\n",
    "    model=q_net,\n",
    "    optim=optim,\n",
    "    action_space=env.action_space,\n",
    "    discount_factor=0.99,\n",
    "    estimation_step=3,\n",
    "    target_update_freq=500,\n",
    ").to(DEVICE)\n",
    "policy.eval()\n",
    "checkpoint_path = \"/root/gym/rl_compexp/save/Blackjack-DQN64/dqn_best.pth\"\n",
    "policy.load_state_dict(torch.load(checkpoint_path,map_location=DEVICE))\n",
    "hidden_outputs = []\n",
    "def hook_fn(module, input, output):\n",
    "    # Assuming output is a tensor, detach and move to CPU\n",
    "    hidden_outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "policy.model.model.model[2].register_forward_hook(hook_fn)\n",
    "\n",
    "weight = policy.model.model.model[4].weight.detach().cpu().t().numpy()\n",
    "state = np.array([21,9,1])\n",
    "print(state.shape)\n",
    "action = policy(Batch(obs=state.reshape(1, -1), info=\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3466,  0.1134], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "0.69992423\n",
      "Batch(\n",
      "    logits: tensor([[0.9306, 0.1792]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
      "    act: array([0]),\n",
      "    state: None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(policy.model.model.model[4].weight[:,9])\n",
    "print(hidden_outputs[0][0][9])\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mean_iou = {}\n",
    "i = 5\n",
    "res = pd.read_csv(f\"result_{i}.csv\")\n",
    "res = res[res[\"iou\"] > 0]\n",
    "res = res.sort_values(by=\"iou\", ascending=False)\n",
    "mean_iou[i] = res[\"iou\"].mean()\n",
    "res.to_csv(f\"result_{i}_parsed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find 4 outputs top connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({48, 37}, {9, 28}) \n",
      "\n",
      "  11 0.5003815\n",
      "  35 0.3611781\n",
      "  20 0.3492526\n",
      "* 48 0.15106824\n",
      "  59 0.13365164\n",
      "  57 0.09165179\n",
      "  23 0.047311936\n",
      "* 37 0.01940344\n",
      "  2 -0.20769313\n",
      "  34 -0.24242833\n",
      "  57 0.90779394\n",
      "  23 0.25493503\n",
      "  20 0.2245203\n",
      "  59 0.18274003\n",
      "  35 0.13949797\n",
      "* 9 0.11335883\n",
      "  11 -0.020975383\n",
      "  2 -0.04273699\n",
      "  34 -0.062015936\n",
      "* 28 -0.11444995\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "i = 5\n",
    "N = 10\n",
    "csv = pd.read_csv(f\"result_{i}_parsed.csv\")\n",
    "output_file = f\"output_{i}.txt\"\n",
    "neurons = []\n",
    "\n",
    "\n",
    "def find_unique_elements(arr1, arr2):\n",
    "    set1, set2,  = set(arr1), set(arr2)\n",
    "    unique1 = set1 - set2 \n",
    "    unique2 = set2 - set1 \n",
    "\n",
    "\n",
    "    return unique1, unique2\n",
    "\n",
    "\n",
    "for label in [\"w_stick\",\"w_hit\"]:\n",
    "    sorted_csv = csv.sort_values(by=label, ascending=False)\n",
    "    top_neurons = sorted_csv.head(N)[\"neuron\"].tolist()\n",
    "    neurons.append(top_neurons)\n",
    "\n",
    "unique_elements = find_unique_elements(neurons[0], neurons[1])\n",
    "print(unique_elements, \"\\n\")\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for i, label in enumerate([\"w_stick\", \"w_hit\"]):\n",
    "        csv = csv.sort_values(by=label, ascending=False)\n",
    "        top_neurons = csv.head(N)[[\"neuron\", label]].to_records(index=False)\n",
    "        print(f\"\\n============\\n{label} top {N}:\", file=f)\n",
    "        for neuron, value in top_neurons:\n",
    "            if neuron in unique_elements[i]:\n",
    "                print(f\"* {neuron} {value}\", file=f)  # Special mark for unique elements\n",
    "                print(f\"* {neuron} {value}\")\n",
    "            else:\n",
    "                print(f\"  {neuron} {value}\", file=f)\n",
    "                print(f\"  {neuron} {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find act & inp pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 1 1 1]\n",
      "(10000,)\n",
      "(1715,)\n",
      "(2127,)\n",
      "(2696,)\n",
      "(3462,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "act = np.load(f\"../save/LunarLander-{NET_NAME}/actions.npy\")\n",
    "obs = np.load(\"../save/lunar.npy\")\n",
    "\n",
    "act_list = []\n",
    "act_idx_list = []\n",
    "max_indices = np.argmax(act, axis=1)\n",
    "print(max_indices)\n",
    "print(max_indices.shape)\n",
    "for i in range(4):\n",
    "    max_row_index = np.where(max_indices == i)[0]\n",
    "    print(max_row_index.shape)\n",
    "    # max_row = act[max_row_index]\n",
    "    act_idx_list.append(max_row_index)\n",
    "    act_list.append(obs[max_row_index])\n",
    "\n",
    "# np.save(f'../save/LunarLander-{NET_NAME}/max_indices.npy', max_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyze specific neuron(DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model and dataset\n",
      "Extract features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:00<00:00, 419.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing quantiles\n",
      "Generating atomic masks\n",
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "from analyze import *\n",
    "\n",
    "ckpt_path = \"../save/ckpt-mlp1024-260.pth\"\n",
    "data_path = \"../save/lunar.npy\"\n",
    "print(\"Load model and dataset\")\n",
    "model, dataset = load_for_analysis(ckpt_path, data_path)\n",
    "weights = model.fc3.weight.t().detach().cpu().numpy()\n",
    "\n",
    "print(\"Extract features\")\n",
    "inputs, features, outputs = extract_feature(model, dataset)\n",
    "outputs = np.array(outputs)\n",
    "if not os.path.exists(settings.RESULT):\n",
    "    # 如果不存在，创建路径\n",
    "    os.makedirs(settings.RESULT)\n",
    "np.save(os.path.join(settings.RESULT, f\"actions.npy\"), outputs)\n",
    "print(\"Computing quantiles\")\n",
    "activations = quantile_features(features)\n",
    "print(\"Generating atomic masks\")\n",
    "feat_masks = gen_feat_mask(inputs)\n",
    "print(feat_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[x, y, Vx, Vy, angle, angular_v, l, r]\n",
    "\n",
    "y_near_ground: <0.4\n",
    "\n",
    "Vy_low: -0.5 ~ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1237,)\n",
      "[54.300285 52.586964 50.26787  52.025227]\n",
      "[ 0.30821085  0.35956365  1.         -0.44264442  0.39747119 -0.08431965\n",
      "  0.          0.        ]\n",
      "0.63623697\n",
      "[ 0.30821085  0.35956365  1.         -0.44264442  0.39747119 -0.08431965\n",
      "  0.          0.        ]\n",
      "[array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "def intersect(x: np.ndarray, y: np.ndarray):\n",
    "    return np.array(list(set(x).intersection(set(y))))\n",
    "\n",
    "\n",
    "vy_low = np.where(feat_masks[:, 7] > 0)[0]\n",
    "y_near_ground = np.where(feat_masks[:, 4] > 0)[0]\n",
    "\n",
    "vy_low_and_y_near_ground = intersect(vy_low, y_near_ground)\n",
    "\n",
    "neuron_act = np.where(activations[:, 630] > 0)[0]\n",
    "res = intersect(neuron_act, act_idx_list[0])\n",
    "res = intersect(res, vy_low_and_y_near_ground)\n",
    "print(res.shape)\n",
    "print(act[res[10]])\n",
    "print(obs[res[10]])\n",
    "print(features[res[10]][630])\n",
    "hook = HookLayer()\n",
    "hook.hook_layer(layer=model.act2)\n",
    "obs_attk = obs[res[10]]\n",
    "obs_attk[2] = 1.0\n",
    "print(obs_attk)\n",
    "act_attk = model(torch.tensor(obs_attk, dtype=torch.float32).to(settings.DEVICE))\n",
    "print(hook.features_blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "tensor([88.7608, 86.5204, 93.6968, 86.7465], device='cuda:3',\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(hook.features_blobs[0][630])\n",
    "print(act_attk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyze attack (PPO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model and dataset\n",
      "Computing quantiles\n",
      "Generating atomic masks\n",
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "from PPO import get_PPO\n",
    "import settings\n",
    "from analyze import *\n",
    "\n",
    "ckpt_path = \"../save/LunarLander-PPO1024/ppo.pth\"\n",
    "data_path = \"../save/lunar.npy\"\n",
    "print(\"Load model and dataset\")\n",
    "inputs, features, outputs, weight, policy = get_PPO(ckpt_path, data_path)\n",
    "\n",
    "\n",
    "if not os.path.exists(settings.RESULT):\n",
    "    # 如果不存在，创建路径\n",
    "    os.makedirs(settings.RESULT)\n",
    "np.save(os.path.join(settings.RESULT, f\"actions.npy\"), outputs)\n",
    "print(\"Computing quantiles\")\n",
    "activations = quantile_features(features)\n",
    "print(\"Generating atomic masks\")\n",
    "feat_masks = gen_feat_mask(inputs)\n",
    "print(feat_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14,)\n",
      "[0.28012165 0.15548752 0.48146957 0.0829213 ]\n",
      "[ 0.9         0.01810535  1.         -0.2070954   0.21621577 -0.67224586\n",
      "  1.          1.        ]\n",
      "0.37163532\n",
      "[[ 0.9         0.01810535  1.         -0.2070954   0.21621577 -0.67224586\n",
      "   1.          1.        ]]\n",
      "(1, 8)\n",
      "-1.70284\n",
      "Batch(\n",
      "    logits: tensor([[1.2064e-01, 8.7784e-01, 1.1213e-03, 3.9262e-04]], device='cuda:3',\n",
      "                   grad_fn=<SoftmaxBackward0>),\n",
      "    act: tensor([1], device='cuda:3'),\n",
      "    state: None,\n",
      "    dist: Categorical(probs: torch.Size([1, 4])),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import tianshou\n",
    "\n",
    "\n",
    "def intersect(x: np.ndarray, y: np.ndarray):\n",
    "    return np.array(list(set(x).intersection(set(y))))\n",
    "\n",
    "\n",
    "not_l_leg = np.where(feat_masks[:, 14] == 0)[0]\n",
    "r_leg = np.where(feat_masks[:, 15] > 0)[0]\n",
    "x_in_centor = np.where(feat_masks[:, 0] > 0)[0]\n",
    "\n",
    "inter = intersect(not_l_leg, r_leg)\n",
    "inter = intersect(x_in_centor, inter)\n",
    "\n",
    "neuron_act = np.where(activations[:, 212] > 0)[0]\n",
    "res = intersect(neuron_act, act_idx_list[2])\n",
    "res = intersect(res, inter)\n",
    "print(res.shape)\n",
    "print(act[res[10]])\n",
    "print(obs[res[10]])\n",
    "print(features[res[10]][212])\n",
    "hook = HookLayer()\n",
    "hook.hook_layer(policy.actor.preprocess.model.model[2])\n",
    "obs_attk = obs[res[10]]\n",
    "obs_attk[0] = 0.9\n",
    "obs_attk[6] = 1.0\n",
    "obs_attk = obs_attk.reshape(1, -1)\n",
    "print(obs_attk)\n",
    "print(obs_attk.shape)\n",
    "act_attk = policy(tianshou.data.Batch(obs=obs_attk, info=\"\"))\n",
    "print(hook.features_blobs[0][0][212])\n",
    "print(act_attk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove useless neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mean_iou = {}\n",
    "i = 5\n",
    "res = pd.read_csv(f\"result_{i}.csv\")\n",
    "res = res[res[\"iou\"] > 0]\n",
    "res = res.sort_values(by=\"iou\", ascending=False)\n",
    "mean_iou[i] = res[\"iou\"].mean()\n",
    "res.to_csv(f\"result_{i}_parsed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find 4 outputs top connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({8, 58, 4, 36}, set(), {54}, set()) \n",
      "\n",
      "  43 3.0032465\n",
      "  50 2.4275603\n",
      "  42 2.1019106\n",
      "  29 2.0634496\n",
      "  11 1.7551857\n",
      "* 58 0.9783437\n",
      "* 4 0.9145632\n",
      "  6 0.5453575\n",
      "* 8 0.32049042\n",
      "* 36 0.1964968\n",
      "  5 2.2308052\n",
      "  41 2.0272539\n",
      "  29 2.0034888\n",
      "  6 1.6569151\n",
      "  43 1.6063035\n",
      "  1 1.2202142\n",
      "  0 0.7341674\n",
      "  11 0.6483451\n",
      "  42 0.52189475\n",
      "  19 0.4329153\n",
      "  50 2.1955557\n",
      "  29 1.8131213\n",
      "  42 1.7148129\n",
      "  0 1.5977954\n",
      "  5 1.4379873\n",
      "  41 1.3720415\n",
      "* 54 1.096387\n",
      "  43 1.0882556\n",
      "  11 0.96671075\n",
      "  1 0.9040145\n",
      "  29 2.427602\n",
      "  5 1.6537093\n",
      "  11 1.5697021\n",
      "  41 1.3866223\n",
      "  43 1.3612376\n",
      "  19 1.2705342\n",
      "  42 1.2181396\n",
      "  50 1.1292526\n",
      "  6 0.5535836\n",
      "  0 0.5304088\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "i = 5\n",
    "N = 10\n",
    "csv = pd.read_csv(f\"result_{i}_parsed.csv\")\n",
    "output_file = f\"output_{i}.txt\"\n",
    "neurons = []\n",
    "\n",
    "\n",
    "def find_unique_elements(arr1, arr2, arr3, arr4):\n",
    "    set1, set2, set3, set4 = set(arr1), set(arr2), set(arr3), set(arr4)\n",
    "    unique1 = set1 - set2 - set3 - set4\n",
    "    unique2 = set2 - set1 - set3 - set4\n",
    "    unique3 = set3 - set1 - set2 - set4\n",
    "    unique4 = set4 - set1 - set2 - set3\n",
    "\n",
    "    return unique1, unique2, unique3, unique4\n",
    "\n",
    "\n",
    "for label in [\"w_nothing\", \"w_left\", \"w_main\", \"w_right\"]:\n",
    "    sorted_csv = csv.sort_values(by=label, ascending=False)\n",
    "    top_neurons = sorted_csv.head(N)[\"neuron\"].tolist()\n",
    "    neurons.append(top_neurons)\n",
    "\n",
    "unique_elements = find_unique_elements(neurons[0], neurons[1], neurons[2], neurons[3])\n",
    "print(unique_elements, \"\\n\")\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for i, label in enumerate([\"w_nothing\", \"w_left\", \"w_main\", \"w_right\"]):\n",
    "        csv = csv.sort_values(by=label, ascending=False)\n",
    "        top_neurons = csv.head(N)[[\"neuron\", label]].to_records(index=False)\n",
    "        print(f\"\\n============\\n{label} top {N}:\", file=f)\n",
    "        for neuron, value in top_neurons:\n",
    "            if neuron in unique_elements[i]:\n",
    "                print(f\"* {neuron} {value}\", file=f)  # Special mark for unique elements\n",
    "                print(f\"* {neuron} {value}\")\n",
    "            else:\n",
    "                print(f\"  {neuron} {value}\", file=f)\n",
    "                print(f\"  {neuron} {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: [[ 1.8729210e-01  6.0042236e-03 -4.5699798e-02 -2.1145369e-01\n",
      "   7.7426016e-02 -1.8042313e-02  0.0000000e+00  1.0000000e+00]\n",
      " [ 1.8670006e-01  2.3698306e-03 -4.0064070e-02 -1.6092648e-01\n",
      "   5.7646915e-02 -3.9668244e-01  0.0000000e+00  1.0000000e+00]\n",
      " [ 1.8610191e-01 -5.7966949e-04 -2.3902554e-02 -1.3010798e-01\n",
      "   2.2112370e-02 -7.1076018e-01  0.0000000e+00  1.0000000e+00]\n",
      " [ 1.8539505e-01 -2.3948455e-03 -2.7449271e-02 -8.0603443e-02\n",
      "  -2.0634277e-02 -8.5502023e-01  0.0000000e+00  1.0000000e+00]\n",
      " [ 1.8455735e-01 -3.9822124e-03 -4.1613456e-02 -7.1656279e-02\n",
      "  -6.2279489e-02 -8.3290499e-01  0.0000000e+00  1.0000000e+00]\n",
      " [ 1.8101224e-01 -1.1202540e-02 -6.4120926e-02 -1.2287087e-01\n",
      "  -2.1799175e-01 -7.1984541e-01  1.0000000e+00  0.0000000e+00]\n",
      " [ 1.8058309e-01 -1.2690344e-02 -2.4191488e-02 -6.9224559e-02\n",
      "  -2.3536010e-01 -3.4526211e-01  1.0000000e+00  0.0000000e+00]\n",
      " [ 1.8080597e-01 -1.3103797e-02  1.9030128e-02 -1.7857213e-02\n",
      "  -2.3203172e-01  6.6588745e-02  1.0000000e+00  0.0000000e+00]\n",
      " [ 1.8130293e-01 -1.2255218e-02  3.5254404e-02  3.9922226e-02\n",
      "  -2.1741113e-01  2.9241121e-01  1.0000000e+00  0.0000000e+00]\n",
      " [ 1.8184166e-01 -1.1916402e-02  3.6265947e-02  1.7554305e-02\n",
      "  -1.9963965e-01  3.5543889e-01  1.0000000e+00  0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "class LogicalExpressionParser:\n",
    "    def __init__(self, operators):\n",
    "        self.operators = operators\n",
    "        self.tokens = []\n",
    "        self.current = 0\n",
    "\n",
    "    def tokenize(self, expression):\n",
    "        \"\"\"将表达式字符串转换为token列表\"\"\"\n",
    "        # 替换括号为带空格的形式，便于分割\n",
    "        expression = expression.replace(\"(\", \" ( \").replace(\")\", \" ) \")\n",
    "        # 分割并移除空字符串\n",
    "        self.tokens = [token for token in expression.split() if token]\n",
    "        self.current = 0\n",
    "\n",
    "    def peek(self):\n",
    "        \"\"\"查看当前token\"\"\"\n",
    "        if self.current < len(self.tokens):\n",
    "            return self.tokens[self.current]\n",
    "        return None\n",
    "\n",
    "    def advance(self):\n",
    "        \"\"\"移动到下一个token\"\"\"\n",
    "        self.current += 1\n",
    "        return self.tokens[self.current - 1]\n",
    "\n",
    "    def parse(self, expression):\n",
    "        \"\"\"解析表达式\"\"\"\n",
    "        self.tokenize(expression)\n",
    "        return self.parse_expression()\n",
    "\n",
    "    def parse_expression(self):\n",
    "        \"\"\"解析表达式\"\"\"\n",
    "        if self.peek() == \"(\":\n",
    "            return self.parse_group()\n",
    "        elif self.peek() == \"NOT\":\n",
    "            self.advance()  # consume NOT\n",
    "            operand = self.parse_expression()\n",
    "            return lambda x: ~operand(x)\n",
    "        else:\n",
    "            # 原子概念\n",
    "            token = self.advance()\n",
    "            return self.operators[token]\n",
    "\n",
    "    def parse_group(self):\n",
    "        \"\"\"解析括号组\"\"\"\n",
    "        self.advance()  # consume (\n",
    "        left = self.parse_expression()\n",
    "\n",
    "        while self.peek() in [\"AND\", \"OR\"]:\n",
    "            operator = self.advance()\n",
    "            right = self.parse_expression()\n",
    "            \n",
    "            if operator == \"AND\":\n",
    "                left = self.create_and(left, right)\n",
    "            else:  # OR\n",
    "                left = self.create_or(left, right)\n",
    "\n",
    "        self.advance()  # consume )\n",
    "        return left\n",
    "\n",
    "    @staticmethod\n",
    "    def create_and(f1, f2):\n",
    "        return lambda x: f1(x) & f2(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_or(f1, f2):\n",
    "        return lambda x: f1(x) | f2(x)\n",
    "\n",
    "def parse_logical_expression(expression, operators):\n",
    "    parser = LogicalExpressionParser(operators)\n",
    "    return parser.parse(expression)\n",
    "\n",
    "lunarv3_operators = {\n",
    "    \"X1\": lambda inp: (inp[:, 0] > -0.25) & (inp[:, 0] < 0),\n",
    "    \"X2\": lambda inp: (inp[:, 0] >= 0) & (inp[:, 0] < 0.25),\n",
    "    \"X3\": lambda inp: (inp[:, 0] > -0.4) & (inp[:, 0] <= -0.25),\n",
    "    \"X4\": lambda inp: (inp[:, 0] < 0.4) & (inp[:, 0] >= 0.25),\n",
    "    \"X5\": lambda inp: (inp[:, 0] <= -0.4),\n",
    "    \"X6\": lambda inp: (inp[:, 0] >= 0.4),\n",
    "    \"Y1\": lambda inp: (inp[:, 1] <= 0.1),\n",
    "    \"Y2\": lambda inp: (inp[:, 1] > 0.1) & (inp[:, 1] <= 0.2),\n",
    "    \"Y3\": lambda inp: (inp[:, 1] > 0.2) & (inp[:, 1] <= 0.3),\n",
    "    \"Y4\": lambda inp: (inp[:, 1] > 0.3) & (inp[:, 1] <= 0.4),\n",
    "    \"Y5\": lambda inp: (inp[:, 1] > 0.4) & (inp[:, 1] <= 0.5),\n",
    "    \"Y6\": lambda inp: (inp[:, 1] > 0.5) & (inp[:, 1] <= 0.7),\n",
    "    \"Y7\": lambda inp: (inp[:, 1] > 0.7),\n",
    "    \"Vx1\": lambda inp: (inp[:, 2] >= -0.1) & (inp[:, 2] < 0),  # Vx low\n",
    "    \"Vx2\": lambda inp: (inp[:, 2] >= 0) & (inp[:, 2] <= 0.1),  # Vx low\n",
    "    \"Vx3\": lambda inp: (inp[:, 2] > 0.1) & (inp[:, 2] <= 0.2),  # Vx slightly positive\n",
    "    \"Vx4\": lambda inp: (inp[:, 2] > 0.2) & (inp[:, 2] <= 0.4),  # Vx moderately positive\n",
    "    \"Vx5\": lambda inp: (inp[:, 2] > 0.4) & (inp[:, 2] <= 1.0),  # Vx high positive\n",
    "    \"Vx6\": lambda inp: (inp[:, 2] < -0.1) & (inp[:, 2] >= -0.2),  # Vx slightly negative\n",
    "    \"Vx7\": lambda inp: (inp[:, 2] < -0.2)\n",
    "    & (inp[:, 2] >= -0.4),  # Vx moderately negative\n",
    "    \"Vx8\": lambda inp: (inp[:, 2] < -0.4) & (inp[:, 2] >= -1.0),\n",
    "    \"Vy1\": lambda inp: (inp[:, 3] >= -0.1) & (inp[:, 3] <= 0.0),  # Vy low downward\n",
    "    \"Vy2\": lambda inp: (inp[:, 3] < -0.1) & (inp[:, 3] >= -0.2),  # Vy slightly downward\n",
    "    \"Vy3\": lambda inp: (inp[:, 3] < -0.2)\n",
    "    & (inp[:, 3] >= -0.4),  # Vy moderately downward\n",
    "    \"Vy4\": lambda inp: (inp[:, 3] < -0.4) & (inp[:, 3] >= -1.0),  # Vy high downward\n",
    "    \"Vy5\": lambda inp: (inp[:, 3] > 0.0) & (inp[:, 3] <= 0.2),  # Vy slightly upward\n",
    "    \"Vy6\": lambda inp: (inp[:, 3] > 0.2) & (inp[:, 3] <= 0.4),  # Vy moderately upward\n",
    "    \"Vy7\": lambda inp: (inp[:, 3] > 0.4) & (inp[:, 3] <= 1.0),\n",
    "    \"A1\": lambda inp: (inp[:, 4] <= -1.0),  # Angle very large (left)\n",
    "    \"A2\": lambda inp: (inp[:, 4] > -1.0)\n",
    "    & (inp[:, 4] <= -0.15),  # Angle moderately large (left)\n",
    "    \"A3\": lambda inp: (inp[:, 4] >= -0.15) & (inp[:, 4] <= 0),  # Angle near 0 (center)\n",
    "    \"A4\": lambda inp: (inp[:, 4] >= 0) & (inp[:, 4] <= 0.15),  # Angle near 0 (center)\n",
    "    \"A5\": lambda inp: (inp[:, 4] > 0.15)\n",
    "    & (inp[:, 4] <= 1.0),  # Angle moderately large (right)\n",
    "    \"A6\": lambda inp: (inp[:, 4] > 1.0),\n",
    "    \"AV1\": lambda inp: (inp[:, 5] <= -0.25),  # Angular velocity very high (left)\n",
    "    \"AV2\": lambda inp: (inp[:, 5] > -0.25)\n",
    "    & (inp[:, 5] <= -0.1),  # Angular velocity moderately high (left)\n",
    "    \"AV3\": lambda inp: (inp[:, 5] >= -0.1)\n",
    "    & (inp[:, 5] <= 0),  # Angular velocity low (center)\n",
    "    \"AV4\": lambda inp: (inp[:, 5] >= 0)\n",
    "    & (inp[:, 5] <= 0.1),  # Angular velocity low (center)\n",
    "    \"AV5\": lambda inp: (inp[:, 5] > 0.1)\n",
    "    & (inp[:, 5] <= 0.25),  # Angular velocity moderately high (right)\n",
    "    \"AV6\": lambda inp: (inp[:, 5] > 0.25),\n",
    "    \"LLeg\": lambda inp: inp[:, 6] == 1,\n",
    "    \"RLeg\": lambda inp: inp[:, 7] == 1,\n",
    "}\n",
    "\n",
    "# 测试表达式\n",
    "test_expression = \"(LLeg OR RLeg)\"\n",
    "\n",
    "# 解析表达式\n",
    "combined_function = parse_logical_expression(test_expression, lunarv3_operators)\n",
    "\n",
    "# 测试结果\n",
    "import numpy as np\n",
    "# 创建测试数据\n",
    "states = np.load(\"states.npy\")\n",
    "\n",
    "\n",
    "# 使用组合函数\n",
    "result = combined_function(states)\n",
    "# 获取 True 的索引\n",
    "indices = np.where(result)[0][:10]\n",
    "print(\"Result:\", states[indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find act & inp pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 1 1 1]\n",
      "(10000,)\n",
      "(1715,)\n",
      "(2127,)\n",
      "(2696,)\n",
      "(3462,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "act = np.load(f\"../save/LunarLander-{NET_NAME}/actions.npy\")\n",
    "obs = np.load(\"../save/lunar.npy\")\n",
    "\n",
    "act_list = []\n",
    "act_idx_list = []\n",
    "max_indices = np.argmax(act, axis=1)\n",
    "print(max_indices)\n",
    "print(max_indices.shape)\n",
    "for i in range(4):\n",
    "    max_row_index = np.where(max_indices == i)[0]\n",
    "    print(max_row_index.shape)\n",
    "    # max_row = act[max_row_index]\n",
    "    act_idx_list.append(max_row_index)\n",
    "    act_list.append(obs[max_row_index])\n",
    "\n",
    "# np.save(f'../save/LunarLander-{NET_NAME}/max_indices.npy', max_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyze specific neuron(DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model and dataset\n",
      "Extract features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:00<00:00, 419.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing quantiles\n",
      "Generating atomic masks\n",
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "from analyze import *\n",
    "\n",
    "ckpt_path = \"../save/ckpt-mlp1024-260.pth\"\n",
    "data_path = \"../save/lunar.npy\"\n",
    "print(\"Load model and dataset\")\n",
    "model, dataset = load_for_analysis(ckpt_path, data_path)\n",
    "weights = model.fc3.weight.t().detach().cpu().numpy()\n",
    "\n",
    "print(\"Extract features\")\n",
    "inputs, features, outputs = extract_feature(model, dataset)\n",
    "outputs = np.array(outputs)\n",
    "if not os.path.exists(settings.RESULT):\n",
    "    # 如果不存在，创建路径\n",
    "    os.makedirs(settings.RESULT)\n",
    "np.save(os.path.join(settings.RESULT, f\"actions.npy\"), outputs)\n",
    "print(\"Computing quantiles\")\n",
    "activations = quantile_features(features)\n",
    "print(\"Generating atomic masks\")\n",
    "feat_masks = gen_feat_mask(inputs)\n",
    "print(feat_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[x, y, Vx, Vy, angle, angular_v, l, r]\n",
    "\n",
    "y_near_ground: <0.4\n",
    "\n",
    "Vy_low: -0.5 ~ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1237,)\n",
      "[54.300285 52.586964 50.26787  52.025227]\n",
      "[ 0.30821085  0.35956365  1.         -0.44264442  0.39747119 -0.08431965\n",
      "  0.          0.        ]\n",
      "0.63623697\n",
      "[ 0.30821085  0.35956365  1.         -0.44264442  0.39747119 -0.08431965\n",
      "  0.          0.        ]\n",
      "[array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "def intersect(x: np.ndarray, y: np.ndarray):\n",
    "    return np.array(list(set(x).intersection(set(y))))\n",
    "\n",
    "\n",
    "vy_low = np.where(feat_masks[:, 7] > 0)[0]\n",
    "y_near_ground = np.where(feat_masks[:, 4] > 0)[0]\n",
    "\n",
    "vy_low_and_y_near_ground = intersect(vy_low, y_near_ground)\n",
    "\n",
    "neuron_act = np.where(activations[:, 630] > 0)[0]\n",
    "res = intersect(neuron_act, act_idx_list[0])\n",
    "res = intersect(res, vy_low_and_y_near_ground)\n",
    "print(res.shape)\n",
    "print(act[res[10]])\n",
    "print(obs[res[10]])\n",
    "print(features[res[10]][630])\n",
    "hook = HookLayer()\n",
    "hook.hook_layer(layer=model.act2)\n",
    "obs_attk = obs[res[10]]\n",
    "obs_attk[2] = 1.0\n",
    "print(obs_attk)\n",
    "act_attk = model(torch.tensor(obs_attk, dtype=torch.float32).to(settings.DEVICE))\n",
    "print(hook.features_blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "tensor([88.7608, 86.5204, 93.6968, 86.7465], device='cuda:3',\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(hook.features_blobs[0][630])\n",
    "print(act_attk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyze attack (PPO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model and dataset\n",
      "Computing quantiles\n",
      "Generating atomic masks\n",
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "from PPO import get_PPO\n",
    "import settings\n",
    "from analyze import *\n",
    "\n",
    "ckpt_path = \"../save/LunarLander-PPO1024/ppo.pth\"\n",
    "data_path = \"../save/lunar.npy\"\n",
    "print(\"Load model and dataset\")\n",
    "inputs, features, outputs, weight, policy = get_PPO(ckpt_path, data_path)\n",
    "\n",
    "\n",
    "if not os.path.exists(settings.RESULT):\n",
    "    # 如果不存在，创建路径\n",
    "    os.makedirs(settings.RESULT)\n",
    "np.save(os.path.join(settings.RESULT, f\"actions.npy\"), outputs)\n",
    "print(\"Computing quantiles\")\n",
    "activations = quantile_features(features)\n",
    "print(\"Generating atomic masks\")\n",
    "feat_masks = gen_feat_mask(inputs)\n",
    "print(feat_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14,)\n",
      "[0.28012165 0.15548752 0.48146957 0.0829213 ]\n",
      "[ 0.9         0.01810535  1.         -0.2070954   0.21621577 -0.67224586\n",
      "  1.          1.        ]\n",
      "0.37163532\n",
      "[[ 0.9         0.01810535  1.         -0.2070954   0.21621577 -0.67224586\n",
      "   1.          1.        ]]\n",
      "(1, 8)\n",
      "-1.70284\n",
      "Batch(\n",
      "    logits: tensor([[1.2064e-01, 8.7784e-01, 1.1213e-03, 3.9262e-04]], device='cuda:3',\n",
      "                   grad_fn=<SoftmaxBackward0>),\n",
      "    act: tensor([1], device='cuda:3'),\n",
      "    state: None,\n",
      "    dist: Categorical(probs: torch.Size([1, 4])),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import tianshou\n",
    "\n",
    "\n",
    "def intersect(x: np.ndarray, y: np.ndarray):\n",
    "    return np.array(list(set(x).intersection(set(y))))\n",
    "\n",
    "\n",
    "not_l_leg = np.where(feat_masks[:, 14] == 0)[0]\n",
    "r_leg = np.where(feat_masks[:, 15] > 0)[0]\n",
    "x_in_centor = np.where(feat_masks[:, 0] > 0)[0]\n",
    "\n",
    "inter = intersect(not_l_leg, r_leg)\n",
    "inter = intersect(x_in_centor, inter)\n",
    "\n",
    "neuron_act = np.where(activations[:, 212] > 0)[0]\n",
    "res = intersect(neuron_act, act_idx_list[2])\n",
    "res = intersect(res, inter)\n",
    "print(res.shape)\n",
    "print(act[res[10]])\n",
    "print(obs[res[10]])\n",
    "print(features[res[10]][212])\n",
    "hook = HookLayer()\n",
    "hook.hook_layer(policy.actor.preprocess.model.model[2])\n",
    "obs_attk = obs[res[10]]\n",
    "obs_attk[0] = 0.9\n",
    "obs_attk[6] = 1.0\n",
    "obs_attk = obs_attk.reshape(1, -1)\n",
    "print(obs_attk)\n",
    "print(obs_attk.shape)\n",
    "act_attk = policy(tianshou.data.Batch(obs=obs_attk, info=\"\"))\n",
    "print(hook.features_blobs[0][0][212])\n",
    "print(act_attk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
